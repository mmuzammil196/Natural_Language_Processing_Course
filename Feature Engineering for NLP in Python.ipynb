{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter no : 1\n",
    "## Basic features and readability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# Basic feature extraction\n",
    "# Number of characters.\n",
    "text = \"I don't know.\"\n",
    "num_char = len(text)\n",
    "print(num_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"don't\", 'know.']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Number of words.\n",
    "words = text.split()\n",
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for count number of words.\n",
    "def word_count(string):\n",
    "    words = string.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for average word length.\n",
    "def avg_word_length(x):\n",
    "    words = x.split\n",
    "    word_length = [len(word) for word in words]\n",
    "    avg_word_length = sum(word_length/len(words))\n",
    "    return (avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special Feature Extraction\n",
    "# Hashtag and Mentions\n",
    "def hashtag_count(string):\n",
    "    words = string.split()\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    return len(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_count(\"@janedoe This is my first tweet! #FirstTweet #Happy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127447</td>\n",
       "      <td>LIVE STREAM VIDEO=&gt; Donald Trump Rallies in Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123642</td>\n",
       "      <td>Muslim Attacks NYPD Cops with Meat Cleaver. Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226970</td>\n",
       "      <td>.@vfpatlas well that's a swella word there (di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138339</td>\n",
       "      <td>RT wehking_pamela: Bobby_Axelrod2k MMFlint don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161610</td>\n",
       "      <td>Жители обстреливаемых районов Донецка проводят...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            content\n",
       "0      127447  LIVE STREAM VIDEO=> Donald Trump Rallies in Co...\n",
       "1      123642  Muslim Attacks NYPD Cops with Meat Cleaver. Me...\n",
       "2      226970  .@vfpatlas well that's a swella word there (di...\n",
       "3      138339  RT wehking_pamela: Bobby_Axelrod2k MMFlint don...\n",
       "4      161610  Жители обстреливаемых районов Донецка проводят..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('russian_tweets.csv')\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.462\n"
     ]
    }
   ],
   "source": [
    "# Create a feature char_count\n",
    "tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "# Print the average character count\n",
    "print(tweets['char_count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We're going to talk — my — a new lecture, just...</td>\n",
       "      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a representation of your brain, and yo...</td>\n",
       "      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's a great honor today to share with you The...</td>\n",
       "      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My passions are music, technology and making t...</td>\n",
       "      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It used to be that if you wanted to get a comp...</td>\n",
       "      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  We're going to talk — my — a new lecture, just...   \n",
       "1  This is a representation of your brain, and yo...   \n",
       "2  It's a great honor today to share with you The...   \n",
       "3  My passions are music, technology and making t...   \n",
       "4  It used to be that if you wanted to get a comp...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/al_seckel_says_our_b...  \n",
       "1  https://www.ted.com/talks/aaron_o_connell_maki...  \n",
       "2  https://www.ted.com/talks/carter_emmart_demos_...  \n",
       "3  https://www.ted.com/talks/jared_ficklin_new_wa...  \n",
       "4  https://www.ted.com/talks/jeremy_howard_the_wo...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted = pd.read_csv('ted.csv')\n",
    "ted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987.1\n"
     ]
    }
   ],
   "source": [
    "# Function that returns number of words in a string\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "# Create a new feature word_count\n",
    "ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "# Print the average word count of the talks\n",
    "print(ted['word_count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns numner of hashtags in a string\n",
    "def count_hashtags(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are hashtags\n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    \n",
    "    # Return number of hashtags\n",
    "    return(len(hashtags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVBklEQVR4nO3df7DddX3n8edLovyK8kM0DYQx2KYqP9ZtiRRl1r0sbqFFDNsp3bRo0eJmd4uKHVwXnN217ZZddkdsFWV3smDLDNQYoyNYtSsTvdtxW6HyY8QQGaJECISEn5GgIuB7/zjfbA+Xe3OP956Te+/H52OGud/z/fV5f7735nU+53PO+ZKqQpLUlhfMdQGSpOEz3CWpQYa7JDXIcJekBhnuktQgw12SGmS4a1aSVJJfmOs65rMk40ne2S2fm+TLQzz3piRj3fIfJrl2iOf+QJKrhnU+7VuG+8+AJFuTvGnCurcn+dqI2x15G/takr9I8iczPb6qrquqXx1WO1V1XFWNz7SevvbGkmybcO7/UlXvnO25NTcMd2kBSrJormvQ/Ga4C4AkFyf5TpInktyZ5F/0bfuFJP8nya4kDyf51ITD35Tk7iSPJfl4el4D/E/g9Ul2J3m8O9eZSW5L8v0k9yX5wwl1/G6S7yV5JMl/nOxVR9++Bya5vNt/V5KvJTmw2/aWbsri8W5a5DV9xz1nKql/lLxnBJvkoiQ7k2xP8o5u2xrgXOD9XZ8+P0Vd/zzJt7uaPgakb9v/fzXTXac/7drZleSbSY6fqp3uWvz7JN8EnkyyaJLrc0CST3W/x1uTvHa6fic5GPgScGTX3u4kR06c5pnmmm5N8r6uD7u6Gg6Y7Ppo3zDctcd3gH8CHAL8EXBtkqXdtv8MfBk4DFgGXDHh2DcDrwNeC/wWcHpVbQb+DfB3VbW4qg7t9n0S+F3gUOBM4N8mORsgybHAlfSCbWlXy1F7qflDwInAG4DDgfcDP0nyi8AngfcCLwO+CHw+yYsGvBY/19f2+cDHkxxWVWuB64D/3vXprIkHJjkC+AzwH4Aj6F3XU6Zo51eBNwK/SO96/EvgkWna+W161+3QqnpmknOuAj5N73r8JfC5JC/cW2er6kng14AHuvYWV9UDE/o1yDX9LeAM4BjgHwFv31u7Gi3D/WfH57oR1+PdKPrK/o1V9emqeqCqflJVnwLuBk7qNj8NvAI4sqp+VFUT59Evq6rHq+pe4KvAP56qiKoar6o7una+SS8w/mm3+TeBz1fV16rqx8B/Aia9+VGSFwC/B1xYVfdX1bNV9bdV9RS9kPxCVd1YVU/TexI4kN6TwCCeBv64qp6uqi8Cu4FXDXjsrwN3VtWGru0/Ax7cSzsvBl4NpKo2V9X2ac7/0aq6r6p+OMX2W/ra/jBwAHDygLXvzSDX9KPd39CjwOfZy9+BRs9w/9lxdlUduuc/4Pf7N3bTIbf3hf/x9Eae0BsRB7i5e1n+exPO3R9ePwAWT1VEkl9J8tUkDyXZRW90v6edI4H79uxbVT8AHpniVEfQC67vTLLtSOB7fef5SXfevb0K6PfIhFHxXvs0Sdv9faj+x/2q6ivAx4CPAzuSrE3ykmnOP+m5Jtve9XtbV9NsDXJNB/470OgZ7iLJK4D/BbwLeGkX/t+imyuuqger6l9V1ZHAvwauzGAff5xs1P2XwA3A0VV1CL15+T1z0tvpTfvsqetA4KVTnPth4EfAz0+y7QF6rzT2nCfA0cD93aofAAf17f9z03Wkz3S3Ud3etTWx7clPVvXRqjoROI7e9My/m6ad6drvb/sF9K7nnimWvfV7uvNOd001zxjuAjiY3j/uhwC6NxCP37MxyTlJ9oTuY92+zw5w3h3Asgnzsi8GHq2qHyU5Cfidvm0bgLOSvKE75o/oezOyXzdy/ATw4e7Nv/2SvD7J/sB64Mwkp3XzzRcBTwF/2x1+O/A73TFn8A/TQoPYAbxyL9u/AByX5DfS+0TLe5jiySPJ67pXMi+k917Ej/iH6zpdO1M5sa/t99Lr99e7bbczdb93AC9NcsgU553ummqeMdxFVd0JXA78Hb1/5CcA/7dvl9cBNyXZTW/UfWFV3TPAqb8CbAIeTPJwt+73gT9O8gS9OfX1fXVsAt4NrKM3An4C2EkvRCbzPuAO4O+BR4H/Brygqu4C3krvjd+HgbOAs7p5fIALu3WP03vz9nMD9GWPq4Fju+mr5x1XVQ8D5wCX0ZtSWsFzr2W/l9B7xfQYvSmPR+jNZU/bzl5cT29+/DHgbcBvdHPksJd+V9W36b3/8d2uzedM5QxwTTXPxP9Zh+arJIvpBdGKAZ9MJHUcuWteSXJWkoO6z15/iN7IfOvcViUtPIa75ptV9N68e4DelMbq8uWl9FNzWkaSGuTIXZIaNC9uPnTEEUfU8uXLZ3z8k08+ycEHHzy8guYR+7Zwtdw/+zY/3HLLLQ9X1csm2zYvwn358uV84xvfmPHx4+PjjI2NDa+gecS+LVwt98++zQ9JvjfVNqdlJKlBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQfPiG6qzdcf9u3j7xV/Y5+1uvezMfd6mJA3CkbskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBg0U7kn+IMmmJN9K8skkByQ5PMmNSe7ufh7Wt/8lSbYkuSvJ6aMrX5I0mWnDPclRwHuAlVV1PLAfsBq4GNhYVSuAjd1jkhzbbT8OOAO4Msl+oylfkjSZQadlFgEHJlkEHAQ8AKwCrum2XwOc3S2vAtZV1VNVdQ+wBThpaBVLkqY1bbhX1f3Ah4B7ge3Arqr6MrCkqrZ3+2wHXt4dchRwX98ptnXrJEn7yLS3/O3m0lcBxwCPA59O8ta9HTLJuprkvGuANQBLlixhfHx8gHInt+RAuOiEZ2Z8/EzNpuZB7d69e5+0Mxda7hu03T/7Nv8Ncj/3NwH3VNVDAEk+C7wB2JFkaVVtT7IU2Nntvw04uu/4ZfSmcZ6jqtYCawFWrlxZY2NjM+7EFdddz+V37Ptb0289d2zkbYyPjzObazOftdw3aLt/9m3+G2TO/V7g5CQHJQlwGrAZuAE4r9vnPOD6bvkGYHWS/ZMcA6wAbh5u2ZKkvZl2uFtVNyXZANwKPAPcRm/EvRhYn+R8ek8A53T7b0qyHriz2/+Cqnp2RPVLkiYx0FxGVX0Q+OCE1U/RG8VPtv+lwKWzK02SNFN+Q1WSGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGDRTuSQ5NsiHJt5NsTvL6JIcnuTHJ3d3Pw/r2vyTJliR3JTl9dOVLkiYz6Mj9I8BfV9WrgdcCm4GLgY1VtQLY2D0mybHAauA44AzgyiT7DbtwSdLUpg33JC8B3ghcDVBVP66qx4FVwDXdbtcAZ3fLq4B1VfVUVd0DbAFOGm7ZkqS9GWTk/krgIeDPk9yW5KokBwNLqmo7QPfz5d3+RwH39R2/rVsnSdpHFg24zy8D766qm5J8hG4KZgqZZF09b6dkDbAGYMmSJYyPjw9QyuSWHAgXnfDMjI+fqdnUPKjdu3fvk3bmQst9g7b7Z9/mv0HCfRuwrapu6h5voBfuO5IsrartSZYCO/v2P7rv+GXAAxNPWlVrgbUAK1eurLGxsZn1ALjiuuu5/I5BujJcW88dG3kb4+PjzObazGct9w3a7p99m/+mnZapqgeB+5K8qlt1GnAncANwXrfuPOD6bvkGYHWS/ZMcA6wAbh5q1ZKkvRp0uPtu4LokLwK+C7yD3hPD+iTnA/cC5wBU1aYk6+k9ATwDXFBVzw69cknSlAYK96q6HVg5yabTptj/UuDSmZclSZoNv6EqSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWrQwOGeZL8ktyX5q+7x4UluTHJ39/Owvn0vSbIlyV1JTh9F4ZKkqf00I/cLgc19jy8GNlbVCmBj95gkxwKrgeOAM4Ark+w3nHIlSYMYKNyTLAPOBK7qW70KuKZbvgY4u2/9uqp6qqruAbYAJw2lWknSQFJV0++UbAD+K/Bi4H1V9eYkj1fVoX37PFZVhyX5GPD1qrq2W3818KWq2jDhnGuANQBLliw5cd26dTPuxM5Hd7HjhzM+fMZOOOqQkbexe/duFi9ePPJ25kLLfYO2+2ff5odTTz31lqpaOdm2RdMdnOTNwM6quiXJ2ADtZZJ1z3sGqaq1wFqAlStX1tjYIKee3BXXXc/ld0zblaHbeu7YyNsYHx9nNtdmPmu5b9B2/+zb/DdIIp4CvCXJrwMHAC9Jci2wI8nSqtqeZCmws9t/G3B03/HLgAeGWbQkae+mnXOvqkuqallVLaf3RulXquqtwA3Aed1u5wHXd8s3AKuT7J/kGGAFcPPQK5ckTWk2cxmXAeuTnA/cC5wDUFWbkqwH7gSeAS6oqmdnXakkaWA/VbhX1Tgw3i0/Apw2xX6XApfOsjZJ0gz5DVVJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaNG24Jzk6yVeTbE6yKcmF3frDk9yY5O7u52F9x1ySZEuSu5KcPsoOSJKeb5CR+zPARVX1GuBk4IIkxwIXAxuragWwsXtMt201cBxwBnBlkv1GUbwkaXLThntVba+qW7vlJ4DNwFHAKuCabrdrgLO75VXAuqp6qqruAbYAJw25bknSXqSqBt85WQ78DXA8cG9VHdq37bGqOizJx4CvV9W13fqrgS9V1YYJ51oDrAFYsmTJievWrZtxJ3Y+uosdP5zx4TN2wlGHjLyN3bt3s3jx4pG3Mxda7hu03T/7Nj+ceuqpt1TVysm2LRr0JEkWA58B3ltV308y5a6TrHveM0hVrQXWAqxcubLGxsYGLeV5rrjuei6/Y+CuDM3Wc8dG3sb4+DizuTbzWct9g7b7Z9/mv4E+LZPkhfSC/bqq+my3ekeSpd32pcDObv024Oi+w5cBDwynXEnSIAb5tEyAq4HNVfXhvk03AOd1y+cB1/etX51k/yTHACuAm4dXsiRpOoPMZZwCvA24I8nt3boPAJcB65OcD9wLnANQVZuSrAfupPdJmwuq6tlhFy5Jmtq04V5VX2PyeXSA06Y45lLg0lnUJUmaBb+hKkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDFs11AQvZ8ou/MPI2LjrhGd4+oZ2tl5058nYlLWyO3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAZ5b5kFaF/c02Yq3tdGWhgcuUtSgwx3SWqQ4S5JDRrZnHuSM4CPAPsBV1XVZaNqS/vOMOf7J7tX/Xzk+wxaiEYyck+yH/Bx4NeAY4HfTnLsKNqSJD3fqEbuJwFbquq7AEnWAauAO0fUnjQyM321slBemUzmZ+3VSv/veF//3kZ1rVNVwz9p8pvAGVX1zu7x24Bfqap39e2zBljTPXwVcNcsmjwCeHgWx89n9m3harl/9m1+eEVVvWyyDaMauWeSdc95FqmqtcDaoTSWfKOqVg7jXPONfVu4Wu6ffZv/RvVpmW3A0X2PlwEPjKgtSdIEowr3vwdWJDkmyYuA1cANI2pLkjTBSKZlquqZJO8C/je9j0J+oqo2jaKtzlCmd+Yp+7Zwtdw/+zbPjeQNVUnS3PIbqpLUIMNdkhq0oMM9yRlJ7kqyJcnFc13PsCQ5OslXk2xOsinJhXNd07Al2S/JbUn+aq5rGbYkhybZkOTb3e/w9XNd07Ak+YPub/JbST6Z5IC5rmk2knwiyc4k3+pbd3iSG5Pc3f08bC5rnKkFG+6N3+LgGeCiqnoNcDJwQUN92+NCYPNcFzEiHwH+uqpeDbyWRvqZ5CjgPcDKqjqe3oclVs9tVbP2F8AZE9ZdDGysqhXAxu7xgrNgw52+WxxU1Y+BPbc4WPCqantV3dotP0EvHI6a26qGJ8ky4EzgqrmuZdiSvAR4I3A1QFX9uKoen9OihmsRcGCSRcBBLPDvr1TV3wCPTli9CrimW74GOHtf1jQsCzncjwLu63u8jYYCcI8ky4FfAm6a41KG6c+A9wM/meM6RuGVwEPAn3fTTlclOXiuixqGqrof+BBwL7Ad2FVVX57bqkZiSVVth95AC3j5HNczIws53Ke9xcFCl2Qx8BngvVX1/bmuZxiSvBnYWVW3zHUtI7II+GXgf1TVLwFPskBf1k/UzT2vAo4BjgQOTvLWua1KU1nI4d70LQ6SvJBesF9XVZ+d63qG6BTgLUm20ptK+2dJrp3bkoZqG7Ctqva80tpAL+xb8Cbgnqp6qKqeBj4LvGGOaxqFHUmWAnQ/d85xPTOykMO92VscJAm9OdvNVfXhua5nmKrqkqpaVlXL6f3OvlJVzYz+qupB4L4kr+pWnUY7t7q+Fzg5yUHd3+hpNPJm8QQ3AOd1y+cB189hLTM2sv8T06jNwS0O9qVTgLcBdyS5vVv3gar64tyVpJ/Cu4HrukHHd4F3zHE9Q1FVNyXZANxK7xNdt7HAv6qf5JPAGHBEkm3AB4HLgPVJzqf3hHbO3FU4c95+QJIatJCnZSRJUzDcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoP+H7LUfwTfmxMkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a feature hashtag_count and display distribution\n",
    "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
    "tweets['hashtag_count'].hist()\n",
    "plt.title('Hashtag count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns number of mentions in a string\n",
    "def count_mentions(string):\n",
    "\t# Split the string into words\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words that are mentions\n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    \n",
    "    # Return number of mentions\n",
    "    return(len(mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a feature mention_count and display distribution\n",
    "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
    "tweets['mention_count'].hist()\n",
    "plt.title('Mention count distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readability tests\n",
    "from textatistic import Textatistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay = '\\nThe gods had condemned Sisyphus to ceaselessly rolling a rock to the top of a mountain, whence the stone would fall back of its own weight. They had thought with some reason that there is no more dreadful punishment than futile and hopeless labor. If one believes Homer, Sisyphus was the wisest and most prudent of mortals. According to another tradition, however, he was disposed to practice the profession of highwayman. I see no contradiction in this. Opinions differ as to the reasons why he became the futile laborer of the underworld. To begin with, he is accused of a certain levity in regard to the gods. He stole their secrets. Egina, the daughter of Esopus, was carried off by Jupiter. The father was shocked by that disappearance and complained to Sisyphus. He, who knew of the abduction, offered to tell about it on condition that Esopus would give water to the citadel of Corinth. To the celestial thunderbolts he preferred the benediction of water. He was punished for this in the underworld. Homer tells us also that Sisyphus had put Death in chains. Pluto could not endure the sight of his deserted, silent empire. He dispatched the god of war, who liberated Death from the hands of her conqueror. It is said that Sisyphus, being near to death, rashly wanted to test his wife\\'s love. He ordered her to cast his unburied body into the middle of the public square. Sisyphus woke up in the underworld. And there, annoyed by an obedience so contrary to human love, he obtained from Pluto permission to return to earth in order to chastise his wife. But when he had seen again the face of this world, enjoyed water and sun, warm stones and the sea, he no longer wanted to go back to the infernal darkness. Recalls, signs of anger, warnings were of no avail. Many years more he lived facing the curve of the gulf, the sparkling sea, and the smiles of earth. A decree of the gods was necessary. Mercury came and seized the impudent man by the collar and, snatching him from his joys, lead him forcibly back to the underworld, where his rock was ready for him. You have already grasped that Sisyphus is the absurd hero. He is, as much through his passions as through his torture. His scorn of the gods, his hatred of death, and his passion for life won him that unspeakable penalty in which the whole being is exerted toward accomplishing nothing. This is the price that must be paid for the passions of this earth. Nothing is told us about Sisyphus in the underworld. Myths are made for the imagination to breathe life into them. As for this myth, one sees merely the whole effort of a body straining to raise the huge stone, to roll it, and push it up a slope a hundred times over; one sees the face screwed up, the cheek tight against the stone, the shoulder bracing the clay-covered mass, the foot wedging it, the fresh start with arms outstretched, the wholly human security of two earth-clotted hands. At the very end of his long effort measured by skyless space and time without depth, the purpose is achieved. Then Sisyphus watches the stone rush down in a few moments toward tlower world whence he will have to push it up again toward the summit. He goes back down to the plain. It is during that return, that pause, that Sisyphus interests me. A face that toils so close to stones is already stone itself! I see that man going back down with a heavy yet measured step toward the torment of which he will never know the end. That hour like a breathing-space which returns as surely as his suffering, that is the hour of consciousness. At each of those moments when he leaves the heights and gradually sinks toward the lairs of the gods, he is superior to his fate. He is stronger than his rock. If this myth is tragic, that is because its hero is conscious. Where would his torture be, indeed, if at every step the hope of succeeding upheld him? The workman of today works everyday in his life at the same tasks, and his fate is no less absurd. But it is tragic only at the rare moments when it becomes conscious. Sisyphus, proletarian of the gods, powerless and rebellious, knows the whole extent of his wretched condition: it is what he thinks of during his descent. The lucidity that was to constitute his torture at the same time crowns his victory. There is no fate that can not be surmounted by scorn. If the descent is thus sometimes performed in sorrow, it can also take place in joy. This word is not too much. Again I fancy Sisyphus returning toward his rock, and the sorrow was in the beginning. When the images of earth cling too tightly to memory, when the call of happiness becomes too insistent, it happens that melancholy arises in man\\'s heart: this is the rock\\'s victory, this is the rock itself. The boundless grief is too heavy to bear. These are our nights of Gethsemane. But crushing truths perish from being acknowledged. Thus, Edipus at the outset obeys fate without knowing it. But from the moment he knows, his tragedy begins. Yet at the same moment, blind and desperate, he realizes that the only bond linking him to the world is the cool hand of a girl. Then a tremendous remark rings out: \"Despite so many ordeals, my advanced age and the nobility of my soul make me conclude that all is well.\" Sophocles\\' Edipus, like Dostoevsky\\'s Kirilov, thus gives the recipe for the absurd victory. Ancient wisdom confirms modern heroism. One does not discover the absurd without being tempted to write a manual of happiness. \"What!---by such narrow ways--?\" There is but one world, however. Happiness and the absurd are two sons of the same earth. They are inseparable. It would be a mistake to say that happiness necessarily springs from the absurd. Discovery. It happens as well that the felling of the absurd springs from happiness. \"I conclude that all is well,\" says Edipus, and that remark is sacred. It echoes in the wild and limited universe of man. It teaches that all is not, has not been, exhausted. It drives out of this world a god who had come into it with dissatisfaction and a preference for futile suffering. It makes of fate a human matter, which must be settled among men. All Sisyphus\\' silent joy is contained therein. His fate belongs to him. His rock is a thing. Likewise, the absurd man, when he contemplates his torment, silences all the idols. In the universe suddenly restored to its silence, the myriad wondering little voices of the earth rise up. Unconscious, secret calls, invitations from all the faces, they are the necessary reverse and price of victory. There is no sun without shadow, and it is essential to know the night. The absurd man says yes and his efforts will henceforth be unceasing. If there is a personal fate, there is no higher destiny, or at least there is, but one which he concludes is inevitable and despicable. For the rest, he knows himself to be the master of his days. At that subtle moment when man glances backward over his life, Sisyphus returning toward his rock, in that slight pivoting he contemplates that series of unrelated actions which become his fate, created by him, combined under his memory\\'s eye and soon sealed by his death. Thus, convinced of the wholly human origin of all that is human, a blind man eager to see who knows that the night has no end, he is still on the go. The rock is still rolling. I leave Sisyphus at the foot of the mountain! One always finds one\\'s burden again. But Sisyphus teaches the higher fidelity that negates the gods and raises rocks. He too concludes that all is well. This universe henceforth without a master seems to him neither sterile nor futile. Each atom of that stone, each mineral flake of that night filled mountain, in itself forms a world. The struggle itself toward the heights is enough to fill a man\\'s heart. One must imagine Sisyphus happy.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Flesch Reading Ease is 81.67\n"
     ]
    }
   ],
   "source": [
    "# Compute the readability scores \n",
    "readability_scores = Textatistic(essay).scores\n",
    "\n",
    "# Print the flesch reading ease score\n",
    "flesch = readability_scores['flesch_score']\n",
    "print(\"The Flesch Reading Ease is %.2f\" % (flesch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "forbes = '\\nThe idea is to create more transparency about companies and individuals that are breaking the law or are non-compliant with official obligations and incentivize the right behaviors with the overall goal of improving governance and market order. The Chinese Communist Party intends the social credit score system to “allow the trustworthy to roam freely under heaven while making it hard for the discredited to take a single step.” Even though the system is still under development it currently plays out in real life in myriad ways for private citizens, businesses and government officials. Generally, higher credit scores give people a variety of advantages. Individuals are often given perks such as discounted energy bills and access or better visibility on dating websites. Often, those with higher social credit scores are able to forgo deposits on rental properties, bicycles, and umbrellas. They can even get better travel deals. In addition, Chinese hospitals are currently experimenting with social credit scores. A social credit score above 650 at one hospital allows an individual to see a doctor without lining up to pay.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvard_law = '\\nIn his important new book, The Schoolhouse Gate: Public Education, the Supreme Court, and the Battle for the American Mind, Professor Justin Driver reminds us that private controversies that arise within the confines of public schools are part of a broader historical arc — one that tracks a range of cultural and intellectual flashpoints in U.S. history. Moreover, Driver explains, these tensions are reflected in constitutional law, and indeed in the history and jurisprudence of the Supreme Court. As such, debates that arise in the context of public education are not simply about the conflict between academic freedom, public safety, and student rights. They mirror our persistent struggle to reconcile our interest in fostering a pluralistic society, rooted in the ideal of individual autonomy, with our desire to cultivate a sense of national unity and shared identity (or, put differently, our effort to reconcile our desire to forge common norms of citizenship with our fear of state indoctrination and overencroachment). In this regard, these debates reflect the unique role that both the school and the courts have played in defining and enforcing the boundaries of American citizenship. \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_digest = '\\nThis week 30 passengers were reportedly injured when a Turkish Airlines flight landing at John F. Kennedy International Airport encountered turbulent conditions. Injuries included bruises, bloody noses, and broken bones. In mid-February, a Delta Airlines flight made an emergency landing to assist three passengers in getting to the nearest hospital after some sudden and unexpected turbulence. Doctors treated 15 passengers after a flight from Miami to Buenos Aires last October for everything from severe bruising to nosebleeds after the plane caught some rough winds over Brazil. In 2016, 23 passengers were injured on a United Airlines flight after severe turbulence threw people into the cabin ceiling. The list goes on. Turbulence has been become increasingly common, with painful outcomes for those on board. And more costly to the airlines, too. Forbes estimates that the cost of turbulence has risen to over $500 million each year in damages and delays. And there are no signs the increase in turbulence will be stopping anytime soon.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_kids = '\\nThat, of course, is easier said than done. The more you eat salty foods, the more you develop a taste for them. The key to changing your diet is to start small. “Small changes in sodium in foods are not usually noticed,” Quader says. Eventually, she adds, the effort will reset a kid’s taste buds so the salt cravings stop. Bridget Murphy is a dietitian at New York University’s Langone Medical Center. She suggests kids try adding spices to their food instead of salt. Eating fruits and veggies and cutting back on packaged foods will also help. Need a little inspiration? Murphy offers this tip: Focus on the immediate effects of a diet that is high in sodium. High blood pressure can make it difficult to be active. “Do you want to be able to think clearly and perform well in school?” she asks. “If you’re an athlete, do you want to run faster?” If you answered yes to these questions, then it’s time to shake the salt habit.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.436002482929858, 20.735401069518716, 11.085587583148559, 5.926785009861934]\n"
     ]
    }
   ],
   "source": [
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute gunning fog index\n",
    "gunning_fog_scores = []\n",
    "for excerpt in excerpts:\n",
    "    readability_scores = Textatistic(excerpt).scores\n",
    "    gunning_fog = readability_scores['gunningfog_score']\n",
    "    gunning_fog_scores.append(gunning_fog)\n",
    "\n",
    "# Print the gunning fog indices\n",
    "print(gunning_fog_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter no : 4\n",
    "## Text preprocessing, POS tagging and NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Lemmatization\n",
    "# Tokenization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', 'do', \"n't\", 'know', 'what', \"I'am\", 'doing', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "string = \"Hello! I don't know what I'am doing here.\"\n",
    "doc = nlp(string)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '!', 'I', 'do', \"n't\", 'know', 'what', 'I', 'be', 'do', 'here', '.']\n"
     ]
    }
   ],
   "source": [
    "string = \"Hello! I don't know what I'm doing here.\"\n",
    "doc = nlp(string)\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettyburg = \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of freedom - and that government of the people, by the people, for the people, shall not perish from the earth.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.', 'Now', 'we', \"'re\", 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', 'We', \"'re\", 'met', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', 'We', \"'ve\", 'come', 'to', 'dedicate', 'a', 'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'gave', 'their', 'lives', 'that', 'that', 'nation', 'might', 'live', '.', 'It', \"'s\", 'altogether', 'fitting', 'and', 'proper', 'that', 'we', 'should', 'do', 'this', '.', 'But', ',', 'in', 'a', 'larger', 'sense', ',', 'we', 'ca', \"n't\", 'dedicate', '-', 'we', 'can', 'not', 'consecrate', '-', 'we', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'The', 'brave', 'men', ',', 'living', 'and', 'dead', ',', 'who', 'struggled', 'here', ',', 'have', 'consecrated', 'it', ',', 'far', 'above', 'our', 'poor', 'power', 'to', 'add', 'or', 'detract', '.', 'The', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', 'we', 'say', 'here', ',', 'but', 'it', 'can', 'never', 'forget', 'what', 'they', 'did', 'here', '.', 'It', 'is', 'for', 'us', 'the', 'living', ',', 'rather', ',', 'to', 'be', 'dedicated', 'here', 'to', 'the', 'unfinished', 'work', 'which', 'they', 'who', 'fought', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', 'It', \"'s\", 'rather', 'for', 'us', 'to', 'be', 'here', 'dedicated', 'to', 'the', 'great', 'task', 'remaining', 'before', 'us', '-', 'that', 'from', 'these', 'honored', 'dead', 'we', 'take', 'increased', 'devotion', 'to', 'that', 'cause', 'for', 'which', 'they', 'gave', 'the', 'last', 'full', 'measure', 'of', 'devotion', '-', 'that', 'we', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'died', 'in', 'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', 'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', 'not', 'perish', 'from', 'the', 'earth', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(gettyburg)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four score and seven year ago our father bring forth on this continent , a new nation , conceive in Liberty , and dedicate to the proposition that all man be create equal . now we be engage in a great civil war , test whether that nation , or any nation so conceive and so dedicated , can long endure . we be meet on a great battlefield of that war . we 've come to dedicate a portion of that field , as a final resting place for those who here give their life that that nation might live . it be altogether fitting and proper that we should do this . but , in a large sense , we ca n't dedicate - we can not consecrate - we can not hallow - this ground . the brave man , live and dead , who struggle here , have consecrate it , far above our poor power to add or detract . the world will little note , nor long remember what we say here , but it can never forget what they do here . it be for we the living , rather , to be dedicate here to the unfinished work which they who fight here have thus far so nobly advanced . it be rather for we to be here dedicate to the great task remain before we - that from these honor dead we take increased devotion to that cause for which they give the last full measure of devotion - that we here highly resolve that these dead shall not have die in vain - that this nation , under God , shall have a new birth of freedom - and that government of the people , by the people , for the people , shall not perish from the earth .\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(gettyburg)\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(' '.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Text cleaning\n",
    "blog = '\\nTwenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "century politic witness alarming rise populism Europe warning sign come UK Brexit Referendum vote swinging way Leave follow stupendous victory billionaire Donald Trump President United States November Europe steady rise populist far right party capitalize Europe Immigration Crisis raise nationalist anti europe sentiment instance include Alternative Germany AfD win seat enter Bundestag upset Germany political order time Second World War success Five Star Movement Italy surge popularity neo nazism neo fascism country Hungary Czech Republic Poland Austria\n"
     ]
    }
   ],
   "source": [
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "# print(lemmas)\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We're going to talk — my — a new lecture, just...</td>\n",
       "      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a representation of your brain, and yo...</td>\n",
       "      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n",
       "      <td>1387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's a great honor today to share with you The...</td>\n",
       "      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My passions are music, technology and making t...</td>\n",
       "      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n",
       "      <td>1548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It used to be that if you wanted to get a comp...</td>\n",
       "      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n",
       "      <td>3480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  We're going to talk — my — a new lecture, just...   \n",
       "1  This is a representation of your brain, and yo...   \n",
       "2  It's a great honor today to share with you The...   \n",
       "3  My passions are music, technology and making t...   \n",
       "4  It used to be that if you wanted to get a comp...   \n",
       "\n",
       "                                                 url  word_count  \n",
       "0  https://www.ted.com/talks/al_seckel_says_our_b...        1704  \n",
       "1  https://www.ted.com/talks/aaron_o_connell_maki...        1387  \n",
       "2  https://www.ted.com/talks/carter_emmart_demos_...         890  \n",
       "3  https://www.ted.com/talks/jared_ficklin_new_wa...        1548  \n",
       "4  https://www.ted.com/talks/jeremy_howard_the_wo...        3480  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      talk new lecture TED I illusion create TED I t...\n",
      "1      representation brain brain break left half log...\n",
      "2      great honor today share Digital Universe creat...\n",
      "3      passion music technology thing combination thi...\n",
      "4      use want computer new program programming requ...\n",
      "                             ...                        \n",
      "495    today I unpack example iconic design perfect s...\n",
      "496    brother belong demographic Pat percent accord ...\n",
      "497    John Hockenberry great Tom I want start questi...\n",
      "498    right moment kill More car internet little mob...\n",
      "499    real problem math education right basically ha...\n",
      "Name: transcript, Length: 500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Part-of-speech tagging\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "string = \"Jame is an amazing guitarist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Jame is an amazing guitarist"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(string)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jame', 'PROPN'),\n",
       " ('is', 'AUX'),\n",
       " ('an', 'DET'),\n",
       " ('amazing', 'ADJ'),\n",
       " ('guitarist', 'NOUN')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotf = 'He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(lotf)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'PRON'),\n",
       " ('found', 'VERB'),\n",
       " ('himself', 'PRON'),\n",
       " ('understanding', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('wearisomeness', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('this', 'DET'),\n",
       " ('life', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('where', 'ADV'),\n",
       " ('every', 'DET'),\n",
       " ('path', 'NOUN'),\n",
       " ('was', 'VERB'),\n",
       " ('an', 'DET'),\n",
       " ('improvisation', 'NOUN'),\n",
       " ('and', 'CCONJ'),\n",
       " ('a', 'DET'),\n",
       " ('considerable', 'ADJ'),\n",
       " ('part', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('one', 'PRON'),\n",
       " ('’s', 'ADV'),\n",
       " ('waking', 'VERB'),\n",
       " ('life', 'NOUN'),\n",
       " ('was', 'AUX'),\n",
       " ('spent', 'VERB'),\n",
       " ('watching', 'VERB'),\n",
       " ('one', 'NUM'),\n",
       " ('’s', 'NOUN'),\n",
       " ('feet', 'NOUN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Returns number of other nouns\n",
    "def nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of other nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title label\n",
       "0           0                       You Can Smell Hillary’s Fear  FAKE\n",
       "1           1  Watch The Exact Moment Paul Ryan Committed Pol...  FAKE\n",
       "2           2        Kerry to go to Paris in gesture of sympathy  REAL\n",
       "3           3  Bernie supporters on Twitter erupt in anger ag...  FAKE\n",
       "4           4   The Battle of New York: Why This Primary Matters  REAL"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines = pd.read_csv('fakenews.csv')\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean no. of proper nouns in real and fake headlines are 2.40 and 4.67 respectively\n"
     ]
    }
   ],
   "source": [
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean no. of other nouns in real and fake headlines are 2.28 and 1.84 respectively\n"
     ]
    }
   ],
   "source": [
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John Doe', 'PERSON'), ('France', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "# 3. Named entity recognition\n",
    "# ner with spacy\n",
    "string = \"John Doe is a software engineer working at Google. He lives in France.\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(string)\n",
    "ner = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "print(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "Mountain View GPE\n"
     ]
    }
   ],
   "source": [
    "# Load the required model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc instance \n",
    "text = 'Sundar is the CEO of Google. Its headquarters is in Mountain View.'\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print all named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = \"\\nIt’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sheryl Sandberg', 'Mark Zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "def find_persons(text):\n",
    "  # Create Doc object\n",
    "    doc = nlp(text)\n",
    "  \n",
    "  # Identify the persons\n",
    "    persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "  # Return persons\n",
    "    return persons\n",
    "\n",
    "print(find_persons(tc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter no : 03\n",
    "## N-Gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Building a bag of words model\n",
    "corpus = pd.Series(['The lion is the king of the jungle.',\n",
    "                   'Lions have timespans of a decade.',\n",
    "                   'The lion is an endangered species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 1 1 0 1 0 3 0]\n",
      " [0 1 0 1 0 0 0 0 1 1 0 0 1]\n",
      " [1 0 1 0 1 0 0 1 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "print(bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 13)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   an  decade  endangered  have  is  jungle  king  lion  lions  of  species  \\\n",
      "0   0       0           0     0   1       1     1     1      0   1        0   \n",
      "1   0       1           0     1   0       0     0     0      1   1        0   \n",
      "2   1       0           1     0   1       0     0     1      0   0        1   \n",
      "\n",
      "   the  timespans  \n",
      "0    3          0  \n",
      "1    0          1  \n",
      "2    1          0  \n"
     ]
    }
   ],
   "source": [
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# Print bow_df\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this anime series starts out great interesting...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>some may go for a film like this but i most as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i ve seen this piece of perfection during the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this movie is likely the worst movie i ve ever...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it ll soon be 10 yrs since this movie was rele...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>this movie turned out to be pretty much what i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>from time to time it s very advisable for the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>ed wood is eclipsed and becomes orson welles t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>well here we have yet another role reversal mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>there is a lot of obvious hype associated with...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  sentiment\n",
       "0    this anime series starts out great interesting...          0\n",
       "1    some may go for a film like this but i most as...          0\n",
       "2    i ve seen this piece of perfection during the ...          1\n",
       "3    this movie is likely the worst movie i ve ever...          0\n",
       "4    it ll soon be 10 yrs since this movie was rele...          1\n",
       "..                                                 ...        ...\n",
       "995  this movie turned out to be pretty much what i...          1\n",
       "996  from time to time it s very advisable for the ...          0\n",
       "997  ed wood is eclipsed and becomes orson welles t...          0\n",
       "998  well here we have yet another role reversal mo...          0\n",
       "999  there is a lot of obvious hype associated with...          0\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a BoW Naive Bayes classifier\n",
    "df_movie = pd.read_csv('movie_reviews_clean.csv')\n",
    "df_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_movie['review'], df_movie['sentiment'], test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 15138)\n",
      "(250, 15138)\n"
     ]
    }
   ],
   "source": [
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(lowercase=False, stop_words='english')\n",
    "\n",
    "# Fit and transform X_train\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of X_train_bow and X_test_bow\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = clf.score(X_test_bow, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.808"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.808\n",
      "The sentiment predicted by the classifier is 0\n"
     ]
    }
   ],
   "source": [
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
    "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>tagline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15602</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31357</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11862</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                        title  \\\n",
       "0    862                    Toy Story   \n",
       "1   8844                      Jumanji   \n",
       "2  15602             Grumpier Old Men   \n",
       "3  31357            Waiting to Exhale   \n",
       "4  11862  Father of the Bride Part II   \n",
       "\n",
       "                                            overview  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...   \n",
       "1  When siblings Judy and Peter discover an encha...   \n",
       "2  A family wedding reignites the ancient feud be...   \n",
       "3  Cheated on, mistreated and stepped on, the wom...   \n",
       "4  Just when George Banks has recovered from his ...   \n",
       "\n",
       "                                             tagline  \n",
       "0                                                NaN  \n",
       "1          Roll the dice and unleash the excitement!  \n",
       "2  Still Yelling. Still Fighting. Still Ready for...  \n",
       "3  Friends are the people who let you be yourself...  \n",
       "4  Just When His World Is Back To Normal... He's ...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movie_tagline = pd.read_csv('movie_overviews.csv')\n",
    "df_movie_tagline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                        \n",
       "1               Roll the dice and unleash the excitement!\n",
       "2       Still Yelling. Still Fighting. Still Ready for...\n",
       "3       Friends are the people who let you be yourself...\n",
       "4       Just When His World Is Back To Normal... He's ...\n",
       "                              ...                        \n",
       "9094                                                     \n",
       "9095    Decorated Officer. Devoted Family Man. Defendi...\n",
       "9096                                                     \n",
       "9097                      A god incarnate. A city doomed.\n",
       "9098              The band you know. The story you don't.\n",
       "Name: tagline, Length: 9099, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_nan_with_space = df_movie_tagline['tagline'].fillna(\"\")\n",
    "replace_nan_with_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                        \n",
       "1               Roll the dice and unleash the excitement!\n",
       "2       Still Yelling. Still Fighting. Still Ready for...\n",
       "3       Friends are the people who let you be yourself...\n",
       "4       Just When His World Is Back To Normal... He's ...\n",
       "                              ...                        \n",
       "9094                                                     \n",
       "9095    Decorated Officer. Devoted Family Man. Defendi...\n",
       "9096                                                     \n",
       "9097                      A god incarnate. A city doomed.\n",
       "9098              The band you know. The story you don't.\n",
       "Name: tagline, Length: 9099, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagline  = replace_nan_with_space\n",
    "tagline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng1, ng2 and ng3 have 6614, 37100 and 76881 features respectively\n"
     ]
    }
   ],
   "source": [
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
    "ng1 = vectorizer_ng1.fit_transform(tagline)\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
    "ng2 = vectorizer_ng2.fit_transform(tagline)\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3))\n",
    "ng3 = vectorizer_ng3.fit_transform(tagline)\n",
    "\n",
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define an instance of MultinomialNB \n",
    "# clf_ng = MultinomialNB()\n",
    "\n",
    "# # Fit the classifier \n",
    "# clf_ng.fit(X_train_ng, y_train)\n",
    "\n",
    "# # Measure the accuracy \n",
    "# accuracy = clf_ng.score(X_test_ng, y_test)\n",
    "# print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# # Predict the sentiment of a negative review\n",
    "# review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
    "# prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
    "# print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(df_movie_tagline['overview'], df_movie['sentiment'], test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 1.606 seconds to complete. The accuracy on the test set is 0.77. The ngram representation had 178240 features.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df_movie['review'], df_movie['sentiment'], test_size=0.5, random_state=42, stratify=df_movie['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range = (1,3))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 1.655 seconds to complete. The accuracy on the test set is 0.77. The ngram representation had 178240 features.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df_movie['review'], df_movie['sentiment'], test_size=0.5, random_state=42, stratify=df_movie['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range = (1, 3))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter no : 4\n",
    "## TF-IDF and similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We're going to talk — my — a new lecture, just...</td>\n",
       "      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a representation of your brain, and yo...</td>\n",
       "      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's a great honor today to share with you The...</td>\n",
       "      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My passions are music, technology and making t...</td>\n",
       "      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It used to be that if you wanted to get a comp...</td>\n",
       "      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  We're going to talk — my — a new lecture, just...   \n",
       "1  This is a representation of your brain, and yo...   \n",
       "2  It's a great honor today to share with you The...   \n",
       "3  My passions are music, technology and making t...   \n",
       "4  It used to be that if you wanted to get a comp...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/al_seckel_says_our_b...  \n",
       "1  https://www.ted.com/talks/aaron_o_connell_maki...  \n",
       "2  https://www.ted.com/talks/carter_emmart_demos_...  \n",
       "3  https://www.ted.com/talks/jared_ficklin_new_wa...  \n",
       "4  https://www.ted.com/talks/jeremy_howard_the_wo...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teds = pd.read_csv('ted.csv')\n",
    "teds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      We're going to talk — my — a new lecture, just...\n",
       "1      This is a representation of your brain, and yo...\n",
       "2      It's a great honor today to share with you The...\n",
       "3      My passions are music, technology and making t...\n",
       "4      It used to be that if you wanted to get a comp...\n",
       "                             ...                        \n",
       "495    Today I'm going to unpack for you three exampl...\n",
       "496    Both myself and my brother belong to the under...\n",
       "497    John Hockenberry: It's great to be here with y...\n",
       "498    What you're doing, right now, at this very mom...\n",
       "499    We've got a real problem with math education r...\n",
       "Name: transcript, Length: 500, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted_trans = teds['transcript']\n",
    "ted_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 29158)\n"
     ]
    }
   ],
   "source": [
    "# Building tf-idf document vectors\n",
    "# tf-idf vectors for TED talks\n",
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted_trans)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity using sklearn.\n",
    "# Import the cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check its work or not.\n",
    "A = (4, 7, 1)\n",
    "B = (5, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score of cosine similarity\n",
    "score = cosine_similarity([A], [B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73881883]]\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Initialize numpy vectors\n",
    "A = np.array([1,3])\n",
    "B = np.array([-2,2])\n",
    "\n",
    "# Compute dot product\n",
    "dot_prod = np.dot(A, B)\n",
    "\n",
    "# Print dot product\n",
    "print(dot_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_cs =  ['The sun is the largest celestial body in the solar system', 'The solar system consists of the sun and eight revolving planets', 'Ra was the Egyptian Sun God', 'The Pyramids were the pinnacle of Egyptian architecture', 'The quick brown fox jumps over the lazy dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus_cs)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix,tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               Toy Story\n",
       "1                                                 Jumanji\n",
       "2                                        Grumpier Old Men\n",
       "3                                       Waiting to Exhale\n",
       "4                             Father of the Bride Part II\n",
       "                              ...                        \n",
       "9094                       The Last Brickmaker in America\n",
       "9095                                               Rustom\n",
       "9096                                         Mohenjo Daro\n",
       "9097                                        Shin Godzilla\n",
       "9098    The Beatles: Eight Days a Week - The Touring Y...\n",
       "Name: title, Length: 9099, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a plot line based recommender\n",
    "# Apply preprocess to ted['transcript']\n",
    "movie = pd.read_csv('movie_overviews.csv')\n",
    "# movie['title'] = movie['title'].apply(preprocess)\n",
    "# print(movie['title'])\n",
    "movie_title = movie['title']\n",
    "movie_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity using sklearn.\n",
    "# Import the linear_kernel\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(movie_title)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = pd.Series(movie_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recommend(text, cosine_sim, indices):\n",
    "#     id = indices[index]\n",
    "#     # Get the pairwsie similarity scores of all books compared to that book, \n",
    "#     # sorting them and getting top 5\n",
    "#     similarity_scores = list(enumerate(cosine_sim[id]))\n",
    "#     similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "#     similarity_scores = similarity_scores[1:6]\n",
    "\n",
    "#     # Get the books index\n",
    "#     books_index = [i[0] for i in similarity_scores]\n",
    "\n",
    "#     # Return the top 5 most similar books using integer-location based indexing (iloc)\n",
    "#     return book_description['name'].iloc[books_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(transcript, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[transcript]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return transcripts.iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(movie_title.index, index=movie_title).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ted transcript recommendation system\n",
    "df_transcripts = pd.read_csv('ted.csv')\n",
    "transcripts = df_transcripts['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      We're going to talk — my — a new lecture, just...\n",
       "1      This is a representation of your brain, and yo...\n",
       "2      It's a great honor today to share with you The...\n",
       "3      My passions are music, technology and making t...\n",
       "4      It used to be that if you wanted to get a comp...\n",
       "                             ...                        \n",
       "495    Today I'm going to unpack for you three exampl...\n",
       "496    Both myself and my brother belong to the under...\n",
       "497    John Hockenberry: It's great to be here with y...\n",
       "498    What you're doing, right now, at this very mom...\n",
       "499    We've got a real problem with math education r...\n",
       "Name: transcript, Length: 500, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_indices = pd.Series(transcripts.index, index=transcripts).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the TfidfVectorizer \n",
    "# tfidf_ted = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "# # Construct the TF-IDF matrix\n",
    "# ted_tfidf_matrix = tfidf_ted.fit_transform(transcripts)\n",
    "\n",
    "# # Generate the cosine similarity matrix\n",
    "# ted_cosine_sim = linear_kernel(ted_tfidf_matrix, ted_tfidf_matrix)\n",
    " \n",
    "# # Generate recommendations \n",
    "# print(get_recommendations(\"We've got a real problem\", ted_cosine_sim, ted_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-ffacbe396a64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Beyond n-grams: word embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_lg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \"\"\"\n\u001b[1;32m---> 51\u001b[1;33m     return util.load_model(\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# Beyond n-grams: word embeddings\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.cli.download(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
